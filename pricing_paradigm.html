<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>A broken pricing paradigm</title>
    <meta name="description" content="Cursor, Anthropic expose cracks in cost structure and resource allocation">
    <meta name="keywords" content="AI, pricing, Cursor, Anthropic, tokens, economics, cost structure">
    <meta name="author" content="Anjali Shrivastava">

    <style>
        body {
            margin: 0;
            transition: opacity 0.2s ease-in;
        }

        .banner {
            text-align: center;
            margin-top: 1rem;
            font-size: 1.25rem;
            color: rgba(0, 0, 0, 0.6);
        }

        .container {
            max-width: 50rem;
            font-family: "Helvetica Neue", sans-serif;
            margin: 3rem auto;
            padding: 0 2rem;
            line-height: 1.6;
            text-align: justify;
        }

        .container p {
            font-family: "Helvetica Neue", sans-serif;
            font-size: 1rem;
        }

        .title {
            margin-top: 2rem;
            margin-bottom: 2rem;
            font-size: 3.5rem;
            font-weight: 800;
            line-height: 1.2;
            font-family: "Helvetica Neue", sans-serif;
        }

        .deckhead {
            font-family: Georgia, serif;
            font-style: italic;
            font-size: 1.5rem;
            font-weight: 500;
            margin-top: -1rem;
            margin-bottom: 2rem;
            color: #666666;
        }

        .author {
            font-family: "Helvetica Neue", sans-serif;
            font-weight: bold;
            font-size: 1.2rem;
            margin-bottom: 0.5rem;
        }

        .author-contact {
            font-family: "Helvetica Neue", sans-serif;
            font-size: 1rem;
            margin-bottom: 1rem;
            color: rgb(51, 51, 51);
        }

        .warning {
            background-color: rgba(255, 215, 0, 0.1);
            border-left: 3px solid rgb(255, 215, 0);
            padding: 1rem;
            margin: 2rem 0;
            font-family: "Helvetica Neue", sans-serif;
            font-size: 0.9rem;
            color: rgb(51, 51, 51);
        }

        .blockquote {
            /* font-style: italic; */
            border-left: 6px solid #333;
            margin-left: 2rem;
            padding-left: 0.8rem;
            color: rgb(51, 51, 51);
            font-family: "Helvetica Neue", sans-serif;
            color:#666666
        }

        .subhead {
            text-align: left;
            line-height: 1.2;
            font-family: "Helvetica Neue", sans-serif;
            font-weight: bold;
            font-size: 1.5rem;
            margin-top: 3rem;
            margin-bottom: 1rem;
        }

        .chapter-head {
            text-align: left;
            line-height: 1.2;
            font-family: "Georgia", serif;
            font-weight: 400;
            font-size: 1.2rem;
            margin-top: 3rem;
            /* margin-bottom: 1rem; */
            color:#666666
        }        

        .section-head {
            text-align: left;
            line-height: 1.2;
            font-family: "Helvetica Neue", sans-serif;
            font-weight: bold;
            font-size: 1.2rem;
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        a {
            margin-bottom: 1rem;
            text-decoration: underline;
            color: rgb(0, 51, 204);
            /* font-size: 1rem; */
            transition: color 0.2s;
        }

        a:hover {
            color: orange;
            cursor: pointer;
        }

        .secondary a {
            margin-bottom: 1rem;
            text-decoration: underline;
            color: rgb(102, 102, 102);
            /* font-size: 1rem; */
            transition: color 0.2s;
        }

        .secondary a:hover {
            color: orange;
            cursor: pointer;
        }        

        #backToTop {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            padding: 0.5rem 1rem;
            font-family: "EB Garamond", serif;
            font-size: 1rem;
            color: white;
            border: none;
            border-radius: 999px;
            cursor: pointer;
            display: none;
            z-index: 1000;
            opacity: 0.8;
            background-color: rgba(0, 0, 0, 0.6);
        }

        #backToTop:hover {
            opacity: 1;
        }

        #scrollLink:hover { 
            color: orange !important; 
        }        

        .table-of-contents {
            margin: 2rem 0;
            font-family: "EB Garamond", serif;
        }

        .table-of-contents h3 {
            font-family: "EB Garamond", serif;
            font-weight: bold;
            font-size: 1.2rem;
            margin-top: 0;
            margin-bottom: 1rem;
            color: #333;
        }

        .table-of-contents ul {
            list-style: none;
            padding-left: 0.5rem;
            margin: 0;
        }

        .table-of-contents li {
            margin-bottom: 0.5rem;
            line-height: 1.4;
        }

        .table-of-contents a {
            color: rgb(0, 51, 204);
            text-decoration: none;
            font-size: 1rem;
            font-family: "EB Garamond", serif;
        }

        .table-of-contents a:hover {
            color: white;
            background-color: rgb(0, 51, 204);
            padding: 2px 4px;
            border-radius: 3px;
        }

        .formula {
            font-family: "Courier New", monospace;
            background-color: rgba(0, 0, 0, 0.05);
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
            font-size: 0.9rem;
        }

        .quote-highlight {
            font-style: italic;
            font-size: 1.1rem;
            color: rgb(51, 51, 51);
            margin: 1.5rem 0;
            padding-left: 1rem;
            border-left: 3px solid rgb(102, 0, 204);
        }

        .image-container {
            text-align: center;
            margin: 2rem 0;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .hero-image {
            width: 100%;
            max-width: 768px;
            height: auto;
            margin: 2rem 0;
        }

        @media (max-width: 768px) {
            .container {
                margin: 0;
                max-width: 100%;
            }
            .title {
                font-size: 2.5rem;
            }
            body {
                font-size: 1.25rem;
                line-height: 1.7;
            }
        }
    </style>
</head>

<body>

<div class="banner" role="banner" style="text-align: center;">
  You can always <a href="https://anjalishriva.com/">go home</a>.
</div>

<div class="container">

<h1 class="title">A broken pricing paradigm</h1>

<h2 class="deckhead">Cursor, Anthropic expose cracks in cost structure and resource allocation</h2>

<!-- <div class="image-container">
    <img src="images/image2.png" alt="Pricing paradigm illustration" class="hero-image">
</div> -->

<div class="author">Anjali Shrivastava</div>
<div class="author-contact">
    <a href="mailto:anjali.shrivastava99@gmail.com">anjali.shrivastava99@gmail.com</a> |
    <a href="https://x.com/anjali_shriva">@anjali_shriva</a>
    <p style="color: rgb(102, 102, 102); margin: 0rem;" class="secondary">
    Published: 8/23 | Last updated: 10/22 | <a  href="https://docs.google.com/document/d/1pAtbkNXb-Myxdi7wgxMGBID9VRjhA7yKY26ubI_TyIQ/edit?usp=sharing">Original doc</a>
    <!-- <a style="color: #999999; margin-bottom: 2rem;" href="https://docs.google.com/document/d/1pAtbkNXb-Myxdi7wgxMGBID9VRjhA7yKY26ubI_TyIQ/edit?usp=sharing">View original draft with community comments and research notes</a>  -->
</p>
</div>




<!-- <p style="text-align: center; font-style: italic; color: rgb(51, 51, 51); margin-bottom: 2rem;">
    (questions, comments, suggested edits on this doc are welcome!)
</p> -->

<div class="warning">
    ⚠️ "AI workloads" refers to open-ended, agentic tasks that can spawn compute via recursive reasoning or tool use. This essay does not cover short-context chat or well-defined workflows where the developer can better bound compute.
</div>

<h1 class="chapter-head">PART I: High variance in AI demand breaks unit economics and resource allocation at scale</h1>

<p>The Cursor-Anthropic pricing drama earlier this year exposed a flaw in how we price and deliver open-ended AI workloads. The implications go far beyond margin squeeze and outages: it is a paradigmatic problem.</p>

<div class="table-of-contents">
    <h3>Table of Contents</h3>
    <ul>
        <li><a href="#token-not-fixed">A token is not a fixed, atomic unit of cost</a></li>
        <li><a href="#variance-scales">And variance scales with usage</a></li>
        <li><a href="#tail-risk">Tail risk compounds at hyperscale</a></li>
        <li><a href="#margins-collapse">And margins collapse as usage grows</a></li>
        <li><a href="#saas-economics">High variance kills traditional SaaS unit economics</a></li>
        <li><a href="#resource-allocation">It's a resource allocation problem</a></li>
        <li><a href="#solutions">How to sidestep the pitfalls?</a></li>
    </ul>
</div>

<p>To recap the facts: the Cursor Pro plan launched as a classic SaaS subscription ($20 for unlimited usage). In mid-June, they <a href="https://techcrunch.com/2025/07/07/cursor-apologizes-for-unclear-pricing-changes-that-upset-users/">reversed</a> course on "unlimited" and charged for usage exceeding the $20 price point. It was later <a href="https://www.wheresyoured.at/anthropic-and-openai-have-begun-the-subprime-ai-crisis/">revealed</a> that Anthropic had raised their prices before, and many concluded this to be the root cause.</p>

<p>Cursor's own explanation: "the hardest requests cost an order of magnitude more than simple ones." Their fix was to pass the heavy-tailed cost to users at retail token price from the cloud APIs.</p>

<p>Anthropic's subsequent <a href="https://techcrunch.com/2025/07/28/anthropic-unveils-new-rate-limits-to-curb-claude-code-power-users/">rate limits</a> are more revealing though, because they control the whole stack. The limits appear motivated by the same magnitude in demand: some Claude Code users <a href="https://www.linkedin.com/posts/chrisparsons_after-burning-332-worth-of-compute-i-discovered-activity-7351213366683598850-Xf3a/">reported</a> burning $400+ worth of tokens on a $20 monthly subscription, based on the per token cost that Anthropic themselves publish.</p>

<p>But the limits Anthropic imposed were not token based, but "usage" based. Instead of a transparent "X tokens per week," users simply hit an unlabeled weekly cap and an equally opaque session-based hourly cap.</p>

<div class="image-container">
    <img src="images/image2.png" alt="Token usage patterns">
</div>

<p>This reveals something important: Anthropic's session-based usage limits and weekly limits are a tacit admission that <strong>the token is not a stable unit of cost, nor compute.</strong></p>

<h1 id="token-not-fixed" class="section-head">A token is not a fixed, atomic unit of cost</h1>

<!-- <p>AI usage is inherently heterogeneous, and introduces high variance that make costs unpredictable. Variance exists in task complexity that users request from AI, and the volume of tasks per user.</p> -->

<!-- <p>But once you see that a token is variable itself, things start to make sense.</p> -->

<p>We only consider token count as the atomic, static meter because we inherited the logic from cloud APIs. But, consider the difference between "Describe a cat" and "Describe a cataclysm."</p>

<p>A token is not a fixed unit of work. The intuition is grounded in model architecture:</p>

<ol>
<li><strong>Attention is quadratic</strong> with respect to current context size. Which means inference APIs are linearly pricing fresh tokens whose true compute cost scales quadratically (via <a href="https://x.com/trickylabyrinth/status/1957731651645038765">@trickylabyrinth</a> on X).</li>

<li><strong>Reasoning loops and tool calls</strong> are triggered by token semantics, not length or count. Changing a single word ("sort" → "optimize") can trigger a longer chain-of-thought loop that continues adding tokens up to the context limit.</li>

<li><strong>Mixture of Expert models</strong> may unexpectedly route each token through a variable number of experts up to the expert limit.</li>
</ol>

<p>Meaning, token count is only a proxy meter for costs, not a causal one. And there's signs that per-token pricing breaks down at scale: both Anthropic and Google charge different rates based on prompt length.</p>

<div class="image-container">
    <img src="images/image7.png" alt="API pricing breakdown">
</div>

<p>To be clear: there is no public trace of a model provider indicating that one token can trigger unbounded compute (and therefore cost). But charging higher API rates for nearly full context windows, and Anthropic choosing to impose request-level caps rather than trust the token meter indicate that one request can explode in compute irrespective of token count. Their hedge is to sell a bigger pile of them and hope the tail stays inside that pile.</p>

<p>That string of evidence is enough to conclude the <strong>true</strong> compute cost per token that Anthropic is paying is variable; but at this point, we don't know if its distribution is predictable or volatile (put a pin in this).</p>

<h1 id="variance-scales" class="section-head">And variance scales with usage</h1>

<p>Cost per user is a composite function of many moving variables. Notably, task volume per user, token volume per task, input token cost and output token cost.</p>

<p>We have established that per token costs for inputs and outputs are variable. But we now have real data points indicating the other cost inputs are highly varied.</p>

<p>When Cursor made its price change, some users <a href="https://www.reddit.com/r/cursor/comments/1lqvl21/cursor_12_and_claude_4_sonnet_rate_limit_is_this/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1">hit the cap</a> within 3-5 prompts: a clear sign of variance in task complexity and cost per task.</p>

<p>When Anthropic's usage limits were introduced, some engineers <a href="https://www.linkedin.com/posts/chrisparsons_after-burning-332-worth-of-compute-i-discovered-activity-7351213366683598850-Xf3a/?utm_medium=ios_app&rcm=ACoAAAAdY_oBwfsBoc-jy9JaIbVb1iBMZBhLdKc&utm_source=social_share_send&utm_campaign=share_via">hit it</a> within 8 days. Some run 8 parallel instances of Claude Code. Anthropic themselves said one user <a href="https://x.com/AnthropicAI/status/1949898511287226425">consumed</a> "tens of thousands" on a $200 plan. These are clear signs of user heterogeneity and variance in task volume per user.</p>

<div class="image-container">
    <img src="images/image1.png" alt="Cost per token variance">
</div>

<p>The underlined variables in our formula likely have fat tails — <strong>so it's all of them.</strong></p>

<p>Traditional SaaS has variable costs too (like hosting, customer support and third-party service costs). But these costs follow the law of large numbers, and are normally distributed at scale. You can set a single subscription price that covers this average cost, plus a comfortable margin to absorb tail risk.</p>

<p>In the case of AI software, aggregate daily cost is a composite of fat tailed moving variables. The law of large numbers assumes finite mean and i.i.d. samples, but AI software has at least one dimension with infinite first moment and non-stationary tails. The sample mean keeps wandering instead of converging.</p>

<p>And crucially, the moves made by Cursor and Anthropic indicate their cost per user curves are not normally distributed. Anthropic's usage limits are said to only <a href="https://x.com/AnthropicAI/status/1949898502688903593">apply</a> to "5% of subscribers."</p>

<div class="image-container">
    <img src="images/image5.png" alt="Cost distribution">
</div>

<p>The fat tail won't average out in this case, because the variance of each of the cost inputs scales with volume instead of being diluted by it, and each of the inputs are unbounded.</p>

<h1 id="tail-risk" class="section-head">Tail risk compounds at hyperscale</h1>

<p><a href="https://sfcompute.com/inference">SF Compute</a>'s pricing section is the first vendor statement I've seen that acknowledges the token is not a fixed unit of cost. Their market-based prices expose the real cost drivers of inference (GPU supply, hardware efficiency etc.), in order to offer cheaper prices to developers.</p>

<p>As usage grows, though, the narrative flips from "cheaper on average" to "probable insolvency." Each layer of the aggregate cost curve is highly variable, and the more you scale, the higher the probability that these tail risks can compound.</p>

<div class="formula">
<strong>Formalization</strong><br><br>
Think of every task as a coin-flip whose "size" is how many tokens it costs (X). Most tasks are small (X is a few hundred tokens), but a rare few are huge (tens of thousands).<br><br>
X is a random variable because it's impossible to know the number of output tokens a task will consume when it's submitted. It's bounded by the context window limit, but the shape of the distribution leading up to that bound is still fat-tailed.<br><br>
Mathematically:<br><br>
Let C be the context window limit, and X be the token count for a single task (so 0 ≤ X ≤ C).<br><br>
P(X > x) ~ k / x^α, where 1 < α < 2.<br><br>
Let m be the total number of users and n be the total number of tasks. The aggregate token count is<br><br>
S(m,n) = ∑(i=1 to mn) Xi (independent draws of X).<br><br>
Consider B as the "break-even token count" (i.e., Total Revenue ÷ Price Per Token). B is the budget — when S(m,n) > B, the token seller loses money.
</div>

<p>As <a href="https://hypersoren.xyz/">Soren Larson</a> put it:</p>
<div class="blockquote">
"Traditionally we expect that when we increase the number of users we get diversification reducing risk. But in the fat-tailed case, there's no diversification - every marginal user is an independent lottery ticket for exploding costs."<br>
</div>

<p>There is a reason Cursor and Anthropic are the first to run into these issues publicly: code is instantly verifiable, and its feedback loop is addictive. There are engineers <a href="https://www.linkedin.com/posts/chrisparsons_after-burning-332-worth-of-compute-i-discovered-activity-7351213366683598850-Xf3a/?utm_medium=ios_app&rcm=ACoAAAAdY_oBwfsBoc-jy9JaIbVb1iBMZBhLdKc&utm_source=social_share_send&utm_campaign=share_via">setting</a> <a href="https://x.com/metaphorician/status/1932836580152287652">alarms</a> for when their limits reset. Some have built <a href="https://x.com/dani_avila7/status/1945268457680883775">dashboards</a> to optimize their burn rate. Codegen is the first domain in AI-native software to reach hyperscale and outrun the underlying GPU supply.</p>

<p>Demand elasticity for code generation follows Jevons' paradox (i.e. for each efficiency gain, overall token consumption <a href="https://labs.adaline.ai/p/token-burnout-why-ai-costs-are-climbing?utm_source=substack&utm_medium=email&utm_content=share">rises</a>). We can see this clearly, as Cursor and Github <a href="https://x.com/azeem/status/1952761539565699565">comprise</a> 45% of Anthropic's inference business.</p>

<p>There is a structural problem here: with elastic and nearly infinite demand, risk compounds. <strong>PMF becomes a liability.</strong></p>

<h1 id="margins-collapse" class="section-head">And margins collapse as usage grows</h1>

<p>Margin collapse is the first and most obvious symptom of the problem. Cursor's repricing exposed poor margins, and we also learned that Replit's <a href="https://www.theinformation.com/articles/ai-native-startups-pass-15-billion-annualized-revenue?utm_campaign=Editorial&utm_content=Newsletter&utm_medium=organic_social&utm_source=twitter&rc=fyv6o0">margins</a> are volatile. And there is <a href="https://www.linkedin.com/posts/chrisparsons_after-burning-332-worth-of-compute-i-discovered-activity-7351213366683598850-Xf3a/?utm_medium=ios_app&rcm=ACoAAAAdY_oBwfsBoc-jy9JaIbVb1iBMZBhLdKc&utm_source=social_share_send&utm_campaign=share_via">ample</a> evidence that Anthropic is losing money on its subscriptions.</p>

<p>As far as Anthropic's inference API business, I find it telling that both Anthropic and OpenAI <a href="https://x.com/edzitron/status/1942263690486767891">raised prices</a> on Cursor before any of this came to light.</p>

<p>Many have <a href="https://hypersoren.xyz/posts/smart-squeeze/">written</a> <a href="https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed">about</a> the <a href="https://docs.google.com/document/d/1q3O7niwoxsyfJ5zSx8dgYzipEgBkUqXzLejQQ-PQNWs/edit?tab=t.0">squeeze</a> <a href="https://techcrunch.com/2025/08/07/the-high-costs-and-thin-margins-threatening-ai-coding-startups/">effect</a>, but now we see it as a natural consequence of the variance issue. And the effect isn't limited to the application layer like Cursor and Replit: both model providers and integrators tie revenue to average tokens, but their costs are driven by highly variable usage.</p>

<p>The <a href="https://x.com/edzitron/status/1942274753265557918">narrative</a> that "Cursor raised prices because Anthropic raised theirs" gets the causality backwards. Cursor's users generated the extreme cost tail, because they had no usage limits in place; Anthropic's response was a downstream response, and Cursor's price hikes were forced re-pricings of the tail risk. Anthropic enforced new weekly limits after Cursor's re-pricings, indicating they still could not keep up with the demand.</p>

<p>Each movement is a pressurizing force. The model providers can either absorb the cost shock, or pass it downstream to integrators. The integrators have the same choice (albeit with less freedom).</p>

<p><strong>Both get squeezed:</strong></p>

<div class="image-container">
    <img src="images/image3.png" alt="Demand elasticity and code generation usage">
</div>

<p>The dynamic is especially concerning for startups who face fat tail risk, and have no margin buffer to absorb it.</p>

<h1 id="saas-economics" class="section-head">High variance kills traditional SaaS unit economics</h1>

<p>Subscriptions misprice intelligence, and much of the industry recognizes this, but now we can rigorously explain why.</p>

<p>Traditional SaaS pricing mirrors the physics of stable software, but AI introduces high variance that breaks each of these laws.</p>

<div class="image-container">
    <img src="images/image6.png" alt="Traditional SaaS vs AI pricing models comparison">
</div>
<p>(this can be its own article. If someone else writes it, I’ll link it here).</p>
<h1 id="resource-allocation" class="section-head">It's a resource allocation problem</h1>

<p>Anthropic endured <a href="https://x.com/shidoxo/status/1948767104557019336">constant outages</a> <a href="https://x.com/mynamebedan/status/1945117071735480741">and</a> service <a href="https://x.com/Krayorn/status/1957499186431070696">interruptions</a>, even after <a href="https://x.com/MFrancis107/status/1948756484201779585">enforcing</a> rate limits. This is inextricable from the fact that a token is not an atomic unit of cost.</p>

<p><strong>Because a token is also not an atomic unit of compute.</strong></p>

<p>Similar to how pricing structures treat a token as a fixed unit of cost, product interfaces treat the prompt and context as a static artifact whose size we can measure. But the inputs are more algorithmic than declarative; they dictate how much compute should be allocated to a given task.</p>

<div class="image-container">
    <img src="images/image4.png" alt="Token compute allocation">
</div>

<p>Soren likened applications like Cursor to <a href="https://x.com/hypersoren/status/1941145741596217554">affiliate marketing</a> and used that metaphor to demonstrate the inevitable <a href="https://hypersoren.xyz/posts/smart-squeeze/">margin squeeze</a>.</p>

<p>But the affiliate metaphor holds all the way down the stack. Each layer is just a pass-through to the scarce resource below it, and the scarcest resource under the current paradigm is GPU capacity and power.</p>

<p>The most straightforward way to lift margin and/or prevent outages is to silently dial down "intelligence": shorter chains, lower-quality responses, cheaper sampling. In late August, many users <a href="https://x.com/TheAhmadOsman/status/1961326485672772040">accused</a> Anthropic of doing exactly that after they saw malformed answers, broken tool calls, and rising latency. Anthropic published an <a href="https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues">incident report</a>, but complaints about <a href="https://www.reddit.com/r/ClaudeAI/comments/1mbsa4e/usage_limits_discussion_megathread_starting_july/?sort=confidence">degraded</a> <a href="https://www.reddit.com/r/ClaudeAI/comments/1mj0eyf/usage_limits_megathread_discussion_report_july_28/">quality</a> and service <a href="https://www.reddit.com/r/ClaudeAI/comments/1o2on6q/claude_code_context_window_issue/">continued</a>.</p>

<p>Soren explained "everyone is executing a brute-force/blunt 'hidden work' optimization: what's the least amount of compute I can get away with using such that my user doesn't churn?"</p>

<p>This is starting to sound like a market for lemons, where quality is unobservable, and price competition drives average quality to the floor.</p>

<p>The problem is that this degradation of service is hidden from users, so they have no incentive to optimize their requests to minimize compute. And users repeatedly <a href="https://www.reddit.com/r/ClaudeAI/comments/1mj0eyf/usage_limits_megathread_discussion_report_july_28/">ask</a> for visibility: "Just show a dashboard with remaining weekly & Opus—stop making us guess."</p>

<p>But providers can't offer this, because costs are unpredictable for them too. It's invisible to everyone until the moment the system <a href="https://www.reddit.com/r/RooCode/comments/1lo5cyd/claude_code_crashes/">crashes</a> without warning, leaving users frustrated and providers blindsided by the bill.</p>

<p>Usage caps are the resource allocator of last resort, but they're blunt. Unless the provider can reject or throttle requests before it starts executing, the cost from the exploding compute still materializes. The choice is who should shoulder it.</p>

<h1 id="solutions" class="section-head">How to sidestep the pitfalls?</h1>

<p>That indulgently long intro gets us to the question: how do you design a pricing model that survives margin squeeze, fat-tailed costs and doesn't lead to surprise service degradation?</p>

<p>To safely cushion from unbounded costs, it must price in the variance or be well above the true cost on average. Ideally by anchoring price to value delivered instead of token cost; but value delivered also happens to be highly variable.</p>

<p>A lot of this sounds dire, but I'm certain there are solutions to the pricing and resource allocation problems beyond usage caps or quietly throttling performance. The following are necessary:</p>

<ol>
<li><strong>We need affordances for users to better describe the problem at hand</strong> such that the system can appropriately allocate resources. Language alone is insufficient.</li>

<li><strong>We need a verification signal</strong> if price becomes task-dependent rather than static. Customers are otherwise incentivized to underreport success, so they can avoid paying.</li>

<li><strong>We need to capture upside to protect against potential infinite downside.</strong> But value realization is often delayed and probabilistic, so we need prediction infrastructure.</li>
</ol>

<p>This might sound out there, but I see a line of sight towards all of these solutions, and will elaborate why and how in future installments. If you're working towards solving any of these problems, please DM or email me. I'd love to stress test my intuition and proposed solutions in these areas.</p>

<hr style="margin: 3rem 0; border: none; border-top: 1px solid #ccc;">

<p style="font-style: italic; text-align: center; color: rgb(102, 102, 102);">
    Soren Larson, @trickylabyrinth and Judah contributed significantly to this piece.
</p>

<p style="font-style: italic; text-align: center; color: rgb(102, 102, 102); font-size: 0.9rem;">
    Thank you to <a href="https://x.com/hypersoren">@hypersoren</a>, <a href="https://x.com/trickylabyrinth">@trickylabyrinth</a> and <a href="https://joodaloop.com/">Judah</a> for their work, and to Jason Harrison, Anthony Crognale, Suraj Srivats, Ade Oshineye and Alex Komoroske for offering feedback on early versions of this. And to Analogue Group, for the push to set out on this expedition.
</p>

<p style="font-style: italic; text-align: center; color: rgb(102, 102, 102); font-size: 0.9rem;">View <a href="https://docs.google.comm/document/d/1pAtbkNXb-Myxdi7wgxMGBID9VRjhA7yKY26ubI_TyIQ/edit?usp=sharing">original draft</a> with community comments and research notes.</p> 
</div>

<footer>
  <p style="font-size: 0.9rem; text-align: center; color: rgba(0, 0, 0, 0.6); margin: 4rem 0;">
    It's never too late to
    <span id="scrollLink" style="opacity: 1; color: #0033cc; text-decoration: underline; cursor: pointer;">
      start over</span>.
  </p>
</footer>

<button id="backToTop" style="display: block;">↑ Top</button>

<script>
// Scroll to top functionality
document.getElementById('backToTop').addEventListener('click', function() {
    window.scrollTo({top: 0, behavior: 'smooth'});
});

document.getElementById('scrollLink').addEventListener('click', function() {
    window.scrollTo({top: 0, behavior: 'smooth'});
});

// Show/hide back to top button
window.addEventListener('scroll', function() {
    const backToTop = document.getElementById('backToTop');
    if (window.pageYOffset > 300) {
        backToTop.style.display = 'block';
    } else {
        backToTop.style.display = 'none';
    }
});
</script>

</body>
</html>