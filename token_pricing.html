<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>A token is not a fixed unit of cost</title>
    <meta name="description" content="High variance breaks unit economics and threatens scale">
    <meta name="keywords" content="AI, pricing, Cursor, Anthropic, tokens, economics, cost structure">
    <meta name="author" content="Anjali Shrivastava">
    <link href="./output.css" rel="stylesheet">
</head>
<style>
    .banner {
        text-align: center;
        margin-top: 1rem;
        font-size: 1.25rem;
        color: rgba(0, 0, 0, 0.6);
    }


    .title {
        margin-top: 2rem;
        margin-bottom: 2rem;
        font-size: 3.5rem;
        font-weight: 800;
        line-height: 1.2;
        font-family: "Helvetica Neue", sans-serif;
    }

    .deckhead {
        font-family: Georgia, serif;
        font-style: italic;
        font-size: 1.5rem;
        font-weight: 500;
        margin-top: -1rem;
        margin-bottom: 2rem;
        color: #666666;
    }

    .container {
        line-height: 1.6;
    }

    .container p {
        font-family: "Helvetica Neue", sans-serif;
        font-size: 1rem;
        margin-bottom: 24px;
    }

    .article-contents a, .footer a {
        margin-bottom: 1rem;
        text-decoration: underline;
        color: rgb(0, 51, 204);
        /* font-size: 1rem; */
        transition: color 0.2s;
    }

    .article-contents a:hover, .footer a:hover {
        color: orange;
        cursor: pointer;
    }

    .banner a {
        margin-bottom: 1rem;
        text-decoration: underline;
        color: rgb(0, 51, 204);
        /* font-size: 1rem; */
        transition: color 0.2s;
    }

    .banner a:hover {
        color: orange;
        cursor: pointer;
    }

    .table-of-contents {
        /* margin: 2rem 0; */
        font-family: "EB Garamond", serif;
    }

    .table-of-contents h3 {
        font-family: "EB Garamond", serif;
        font-weight: bold;
        font-size: 1.2rem;
        margin-top: 0;
        margin-bottom: 0.5rem;
        color: #333;
    }

    .table-of-contents ul {
        list-style: none;
        padding-left: 0.5rem;
        margin: 0;
    }

    .table-of-contents li {
        margin-bottom: 0.5rem;
        line-height: 1.4;
    }

    .table-of-contents a {
        color: rgb(0, 51, 204);
        text-decoration: none;
        font-size: 1rem;
        font-family: "EB Garamond", serif;
    }

    .table-of-contents a:hover {
        color: white;
        background-color: rgb(0, 51, 204);
        padding: 2px 4px;
        border-radius: 3px;
    }

    .article-metadata {
        font-family: "EB Garamond", serif;
    }

    .section-head {
        text-align: left;
        line-height: 1.2;
        font-family: "Helvetica Neue", sans-serif;
        font-weight: bold;
        font-size: 1.2rem;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }

    .formula {
        font-family: "Courier New", monospace;
        background-color: rgba(0, 0, 0, 0.05);
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 4px;
        font-size: 0.9rem;
    }

    .blockquote {
        /* font-style: italic; */
        border-left: 6px solid #333;
        margin-left: 2rem;
        padding-left: 0.8rem;
        color: rgb(51, 51, 51);
        font-family: "Helvetica Neue", sans-serif;
        color: #666666
    }

#backToTop {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            padding: 0.5rem 1rem;
            font-family: "EB Garamond", serif;
            font-size: 1rem;
            color: white;
            border: none;
            border-radius: 999px;
            cursor: pointer;
            display: none;
            z-index: 1000;
            opacity: 0.8;
            background-color: rgba(0, 0, 0, 0.6);
        }

        #backToTop:hover {
            opacity: 1;
        }

        #scrollLink:hover { 
            color: orange !important; 
        }        

</style>

<body>
    <div class="mx-auto max-w-[1200px]">
        <div class="banner" role="banner" style="text-align: center;">
            You can always <a href="https://anjalishriva.com/">go home</a>.
        </div>

        <div class="container max-w-[1100px] mx-auto mt-8 px-4 xl:max-w-[1125px]">
            <div class="w-fit text-center mx-auto">
                <h1 id="intro" class="title">A token is not a fixed unit of cost</h1>
                <h2 class="deckhead">Variance in usage creates an interconnected pricing and scaling issue</h2>
            </div>
            <div class="md:flex">
                <div class="max-w-[190px] flex-grow-0 flex-shrink-0 hidden md:block">
                    <div
                        class="article-metadata leading-normal tracking-tighter sticky text-center text-proseBody top-5 dark:text-proseInvertBody">
                        <div class="border-b px-4 pb-4 dark:border-slate-700">
                            <div class="font-bold mb-0.5 text-proseLinks dark:text-proseInvertLinks">Article</div>
                            <div>
                                <a href="https://anjalishriva.com/token_pricing">
                                    A token is not a fixed unit of cost
                                </a>
                            </div>
                        </div>


                        <div class="border-b flex items-stretch text-center dark:border-slate-700">
                            <div class="flex-grow dark:border-slate-700">
                                <div class="flex h-full items-center">
                                    <div class="px-4 py-4 border-r w-[50%]">
                                        <div class="font-bold mb-0.5 text-proseLinks dark:text-proseInvertLinks">
                                            Published
                                        </div>
                                        <div class="leading-tight">August 23, 2025</div>
                                    </div>

                                    <div class="px-4 py-4 w-[50%]">
                                        <div class="font-bold mb-0.5 text-proseLinks dark:text-proseInvertLinks">
                                            Revision
                                        </div>
                                        <div class="leading-tight">November 16, 2025</div>
                                    </div>
                                </div>
                            </div>
                            <!-- <div class="flex-grow h-full min-w-0">
                                <div class="flex items-center">
                                    <div class="px-4 py-4">
                                        <div class="font-bold mb-0.5 text-proseLinks dark:text-proseInvertLinks">
                                            Revision
                                        </div>
                                        <div class="leading-tight">November 16, 2025</div>
                                    </div>
                                </div>
                            </div> -->
                        </div>


                        <div class="border-b px-4 py-4 dark:border-slate-700">
                            See comments, research notes and earlier drafts on <strong><a
                                    href="https://docs.google.com/document/d/1pAtbkNXb-Myxdi7wgxMGBID9VRjhA7yKY26ubI_TyIQ/edit?usp=sharing"
                                    class="text-proseLinks dark:text-proseInvertLinks">Google Docs</a></strong>.
                        </div>


                        <div class="italic px-4 py-4">
                            <p class="mb-2">I'm on Twitter at <a href="https://twitter.com/anjali_shriva"
                                    class="font-bold text-proseLinks dark:text-proseInvertLinks">@anjali_shriva</a>.</p>
                            <p>Email at <a href="mailto:anjali.shrivastava99@gmail.com"
                                    class="font-bold text-proseLinks dark:text-proseInvertLinks">anjali.shrivastava99<br>@gmail.com</a>.
                            </p>
                        </div>
                    </div>
                </div>

                <div class="font-bold my-1 text-proseLinks text-sm dark:text-proseInvertLinks md:hidden">
                    Published August&nbsp;23,&nbsp;2025
                </div>

                <div class="pb-8 md:border-l md:flex-grow md:min-w-0 dark:border-slate-700 md:pl-8 xl:pr-8">

                    <div class="hyphens-auto
                max-w-none
                prose prose-lg dark:prose-invert
                prose-a:border-b prose-a:border-slate-500 prose-a:font-sans prose-a:no-underline
                prose-blockquote:font-normal prose-blockquote:text-slate-800 dark:prose-blockquote:text-slate-300
                hover:prose-a:border-b-slate-200
                prose-code:prose-a:text-sm prose-code:prose-p:text-sm
                prose-img:w-full
                prose-h1:font-bold prose-h1:text-base
                prose-h2:text-base prose-h2:mb-1 prose-a:prose-h2:border-b-0 prose-a:prose-h2:font-bold
                prose-h3:text-sm prose-h3:mb-1 prose-a:prose-h3:border-b-0 prose-a:prose-h3:font-bold
                prose-h4:text-sm prose-h4:mb-1 prose-a:prose-h4:border-b-0 prose-a:prose-h4:font-bold
                prose-hr:my-10 prose-hr:md:-mx-[2rem] prose-hr:md:w-[calc(100%+4rem)] dark:prose-hr:border-slate-700
                prose-p:font-serif
                prose-p:prose-blockquote:font-sans
                prose-pre:leading-[1.15rem] prose-pre:text-[0.9rem]
                prose-strong:font-sans dark:prose-strong:text-slate-100
                prose-ol:font-serif
                prose-ul:font-serif
                article-contents
                ">
                        <div class="table-of-contents md:hidden my-4">
                            <h3>Table of Contents</h3>
                            <ul>
                                <li><a href="#intro">Intro</a></li>
                                <li><a href="#token-not-fixed">A token is not a fixed, atomic unit of cost</a></li>
                                <li><a href="#variance-scales">And tail risk compounds with usage</a></li>
                                <li><a href="#tail-risk">Margins collapse at hyperscale</a></li>
                                <li><a href="#tail-risk" class="ml-[20px] text-xs">Formalization</a></li>
                                <li><a href="#margins-collapse">High variance forces startups to constrain demand</a></li>
                                <li><a href="#saas-economics">Unpredictable costs kill SaaS-style margins</a>
                                </li>
                                <li><a href="#resource-allocation">Scaling AI and improving unit economics require the same fix</a></li>
                                <li><a href="#solutions">Towards a new resource allocation (and pricing) paradigm</a></li>
                                <li><a href="#solutions">Acknowledgements</a></li>
                            </ul>
                        </div>
                        <p>
                            The Cursor-Anthropic pricing drama earlier this year exposed a flaw in how we price and
                            deliver open-ended AI
                            workloads. The implications go far beyond margin squeeze and outages: it is a paradigmatic
                            scaling problem,
                            rooted in the physics of how large language models work.
                        </p>
                        <p>
                            To recap the facts: the Cursor Pro plan launched as a classic SaaS subscription ($20 for
                            unlimited usage). In mid-June, they <a
                                href="https://techcrunch.com/2025/07/07/cursor-apologizes-for-unclear-pricing-changes-that-upset-users/">reversed</a>
                            course on "unlimited" and charged for usage exceeding the $20 price point. It was later <a
                                href="https://www.wheresyoured.at/anthropic-and-openai-have-begun-the-subprime-ai-crisis/">revealed</a>
                            that Anthropic had raised their prices before, and many concluded this to be the root cause.
                        </p>

                        <!-- <p>Cursor's own explanation: "the hardest requests cost an order of magnitude more than simple
                            ones." Their fix was to pass the heavy-tailed cost to users at retail token price from the
                            cloud APIs.</p> -->

                        <p>Anthropic's subsequent <a
                                href="https://techcrunch.com/2025/07/28/anthropic-unveils-new-rate-limits-to-curb-claude-code-power-users/">rate
                                limits</a> are more revealing though, because they control the whole stack. The limits
                            appear motivated by the same magnitude in demand: some Claude Code users <a
                                href="https://www.linkedin.com/posts/chrisparsons_after-burning-332-worth-of-compute-i-discovered-activity-7351213366683598850-Xf3a/">reported</a>
                            burning $400+ worth of tokens on a $20 monthly subscription, based on the per token cost
                            that Anthropic themselves publish.</p>

                        <p>But the limits Anthropic imposed were not token based, but "usage" based. Instead of a
                            transparent "X tokens per week," users simply hit an unlabeled weekly cap and an equally
                            opaque session-based hourly cap.</p>
                        <div class="image-container">
                            <img src="images/image2.png" alt="Token usage patterns">
                        </div>
                        <p>This reveals something important: Anthropic's session-based usage limits and weekly limits
                            are a tacit admission that <strong>the token is not a stable unit of cost, nor
                                compute.</strong></p>

                        <h1 id="token-not-fixed" class="section-head">A token is not a fixed, atomic unit of cost</h1>

                        <!-- <p>AI usage is inherently heterogeneous, and introduces high variance that make costs unpredictable. Variance exists in task complexity that users request from AI, and the volume of tasks per user.</p> -->

                        <!-- <p>But once you see that a token is variable itself, things start to make sense.</p> -->

                        <p>We only consider token count as the static linear meter because we inherited the logic from
                            inference APIs. But,
                            a token does not represent a fixed unit of work.
                        </p>

                        <p>
                            This is obvious to anyone who works in inference, but if you’re used to calculating compute
                            budgets based on linear API rates, it takes a second to sink in.
                        </p>

                        <p>The intuition is grounded in the autoregressive nature of the transformer:
                            Attention is quadratic with respect to current context size. (via <a
                                href="https://x.com/trickylabyrinth/status/1957731651645038765">@trickylabyrinth</a> on
                            X).
                        </p>

                        <p>
                            In layman&apos;s terms, the language model is looking at every previous token in the context
                            window before generating a new token, which means inference APIs are linearly pricing fresh
                            tokens whose compute cost scales quadratically.
                        </p>

                        <p>
                            The scaling law for compute is likely not purely quadratic, given optimizations like caching
                            and compacting context. But no matter what, the underlying compute cost per token grows with
                            context length. The Nth token in a conversation is an order of magnitude more expensive than
                            the first.
                        </p>

                        <p>There's signs that per-token pricing breaks down at scale: both Anthropic and Google charge
                            different rates based on prompt length.</p>

                        <div class="image-container">
                            <img src="images/image7.png" alt="API pricing breakdown">
                        </div>

                        <p>Charging higher API rates for nearly full context windows, and Anthropic choosing to impose
                            request-level caps rather than trust the token meter for rate limiting indicate that one
                            request can explode in compute, irrespective of token count. Their hedge is to sell a bigger
                            pile of them and hope the tail stays inside that pile.</p>

                        <p>A token is variable in cost. </p>

                        <h1 id="variance-scales" class="section-head">And tail risk compounds with usage</h1>

                        <p>
                            We have established that per token costs for inputs and outputs are variable. But the market
                            shift towards agentic loops exposed that every cost input in AI-native software is highly
                            varied.
                        </p>

                        <p>Cost per user is a composite function of many moving variables. Notably, task volume per
                            user, token volume per task, input token cost and output token cost.</p>


                        <p>When Cursor made its price change, some users <a
                                href="https://www.reddit.com/r/cursor/comments/1lqvl21/cursor_12_and_claude_4_sonnet_rate_limit_is_this/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1">hit
                                the cap</a> within 3-5 prompts: a clear sign of variance in task complexity and cost per
                            task.</p>

                        <p>When Anthropic's usage limits were introduced, some engineers <a
                                href="https://www.linkedin.com/posts/chrisparsons_after-burning-332-worth-of-compute-i-discovered-activity-7351213366683598850-Xf3a/?utm_medium=ios_app&rcm=ACoAAAAdY_oBwfsBoc-jy9JaIbVb1iBMZBhLdKc&utm_source=social_share_send&utm_campaign=share_via">hit
                                it</a> within 8 days. Some run 8 parallel instances of Claude Code. Anthropic themselves
                            said one user <a href="https://x.com/AnthropicAI/status/1949898511287226425">consumed</a>
                            "tens of thousands" on a $200 plan. These are clear signs of user heterogeneity and variance
                            in task volume per user.</p>

                        <div class="image-container">
                            <img src="images/image1.png" alt="Cost per token variance">
                        </div>

                        <!-- <p>The underlined variables in our formula likely have fat tails — <strong>so it's all of them.</strong></p> -->

                        <p>
                            Traditional SaaS has variable costs too (like hosting, customer support and third-party
                            service costs). But these costs follow the law of large numbers, and are normally
                            distributed at scale. You can set a single subscription price that covers this average cost,
                            plus a comfortable margin to absorb tail risk.
                        </p>

                        <p>
                            In the case of AI software, it is likely that these variable costs are fat tailed. The law
                            of large numbers assumes finite mean and i.i.d. samples, but AI software has at least one
                            dimension with infinite first moment and non-stationary tails. The sample mean keeps
                            wandering instead of converging.
                        </p>

                        <p>
                            So let’s assume their aggregate daily cost is a composite of fat tailed moving variables.
                            Cursor and Anthropic actively constraining demand, when neither have shifted towards
                            “profit-maximizing” strategies yet, support this.
                            Anthropic's usage limits are said to only <a
                                href="https://x.com/AnthropicAI/status/1949898502688903593">apply</a> to "5% of
                            subscribers."
                        </p>

                        <div class="image-container">
                            <img src="images/image5.png" alt="Cost distribution">
                        </div>

                        <p>The fat tail won't average out in this case, because the variance of each of the cost inputs
                            scales with volume instead of being diluted by it, and each of the inputs are unbounded.</p>

                        <h1 id="tail-risk" class="section-head">Margins collapse at hyperscale</h1>
                        <p>Margin collapse is the first and most obvious symptom of the problem. Cursor's repricing
                            exposed poor margins, and we also learned that Replit's <a
                                href="https://www.theinformation.com/articles/ai-native-startups-pass-15-billion-annualized-revenue?utm_campaign=Editorial&utm_content=Newsletter&utm_medium=organic_social&utm_source=twitter&rc=fyv6o0">margins</a>
                            are volatile. And there is <a
                                href="https://www.linkedin.com/posts/chrisparsons_after-burning-332-worth-of-compute-i-discovered-activity-7351213366683598850-Xf3a/?utm_medium=ios_app&rcm=ACoAAAAdY_oBwfsBoc-jy9JaIbVb1iBMZBhLdKc&utm_source=social_share_send&utm_campaign=share_via">ample</a>
                            evidence that Anthropic is losing money on its subscriptions.</p>

                        <p>
                            Each layer of the aggregate cost curve is highly variable, and the more you scale, the
                            higher the probability that these tail risks can compound.
                        </p>

                        <div class="formula">
                            <strong>Formalization</strong><br><br>
                            Think of every task as a coin-flip whose "size" is how many tokens it costs (X). Most tasks
                            are small (X is a few hundred tokens), but a rare few are huge (tens of thousands).<br><br>
                            X is a random variable because it's impossible to know the number of output tokens a task
                            will consume when it's submitted. It's bounded by the context window limit, but the shape of
                            the distribution leading up to that bound is still fat-tailed.<br><br>
                            Mathematically:<br><br>
                            Let C be the context window limit, and X be the token count for a single task (so 0 ≤ X ≤
                            C).<br><br>
                            P(X > x) ~ k / x^α, where 1 < α < 2.<br><br>
                                Let m be the total number of users and n be the total number of tasks. The aggregate
                                token count is<br><br>
                                S(m,n) = ∑(i=1 to mn) Xi (independent draws of X).<br><br>
                                Consider B as the "break-even token count" (i.e., Total Revenue ÷ Price Per Token). B is
                                the budget — when S(m,n) > B, the token seller loses money. For large budgets B (but
                                still « C), the one-big-jump principle for heavy-tailed sums gives:<br><br>
                                P(S(m,n) > B) ~ (mn) ⋅ P(X > B).<br><br>
                                Meaning the probability of users' token usage exceeding the budget scales linearly with
                                the number of draws. Said simply: more users and more tasks → more risk of financial
                                loss.<br><br>
                                Formally, applying scaling by a factor λ:<br><br>
                                P(S(λm, λn) > B) ~ λmλn ⋅ P(S(m,n) > B).<br><br>
                                Risk grows linearly with total tasks, whether those tasks come from new users or from
                                existing users adding one more task: exactly the opposite of the law of large numbers
                                safety net you'd expect at hyperscale.<br><br>
                                <strong>Plugging in Plausible Numbers</strong><br><br>
                                α = 1.5<br>
                                Context cap C = 200,000 tokens<br>
                                Mean μ = 2,000 tokens (choose k ≈ 63 to match)<br>
                                Daily budget B = 60,000 tokens per user-day<br><br>
                                <strong>Single-Task Exceedance</strong><br><br>
                                P(a task costs more than 60,000 tokens) = P(X > 60,000) ≈ 63 / 60,000^1.5 ≈ 4.3 ×
                                10^−7<br><br>
                                <strong>Aggregate Exceedance at m Users, n Tasks Each</strong><br><br>
                                1,000 users × 30 tasks → 30,000 draws<br>
                                P("loss") ≈ 30,000 × 4.3 × 10^−7 ≈ 1.3%<br><br>
                                10,000 users × 300 tasks → 3,000,000 draws<br>
                                P("loss") ≈ 3,000,000 × 4.3 × 10^−7 ≈ 100% (near-certain loss)
                        </div>

                        <p>As <a href="https://hypersoren.xyz/">Soren Larson</a> put it:</p>
                        <p class="blockquote">
                            "Traditionally we expect that when we increase the number of users we get diversification
                            reducing risk. But in the fat-tailed case, there's no diversification - every marginal user
                            is an independent lottery ticket for exploding costs."<br>
                        </p>

                        <p>
                            There is a reason Cursor and Anthropic are the first to run into these issues publicly: code
                            is instantly verifiable, and its feedback loop is addictive. There are engineers <a
                                href="https://www.linkedin.com/posts/chrisparsons_after-burning-332-worth-of-compute-i-discovered-activity-7351213366683598850-Xf3a/?utm_medium=ios_app&rcm=ACoAAAAdY_oBwfsBoc-jy9JaIbVb1iBMZBhLdKc&utm_source=social_share_send&utm_campaign=share_via">setting</a>
                            <a href="https://x.com/metaphorician/status/1932836580152287652">alarms</a> for when their
                            limits reset.
                            Some have built <a
                                href="https://x.com/dani_avila7/status/1945268457680883775">dashboards</a> to optimize
                            their burn rate.
                            Codegen is the first domain in AI-native software to reach hyperscale and outrun the
                            underlying GPU supply.
                        </p>

                        <p>Demand elasticity for code generation follows Jevons' paradox (i.e. for each efficiency gain,
                            overall token consumption <a
                                href="https://labs.adaline.ai/p/token-burnout-why-ai-costs-are-climbing?utm_source=substack&utm_medium=email&utm_content=share">rises</a>).
                            We can see this clearly, as Cursor and Github <a
                                href="https://x.com/azeem/status/1952761539565699565">comprise</a> 45% of Anthropic's
                            inference business.</p>

                        <p>There is a structural problem here: with elastic and nearly infinite demand, risk compounds.
                            <strong>Product-Market Fit becomes a liability.</strong></p>

                        <h1 id="margins-collapse" class="section-head">High variance forces startups to constrain demand
                        </h1>

                        <p>Many have <a href="https://hypersoren.xyz/posts/smart-squeeze/">written</a> <a
                                href="https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed">about</a>
                            the <a
                                href="https://docs.google.com/document/d/1q3O7niwoxsyfJ5zSx8dgYzipEgBkUqXzLejQQ-PQNWs/edit?tab=t.0">squeeze</a>
                            <a
                                href="https://techcrunch.com/2025/08/07/the-high-costs-and-thin-margins-threatening-ai-coding-startups/">effect</a>,
                            but now we see it as a natural consequence of the variance issue. And the effect isn't
                            limited to the application layer like Cursor and Replit: both model providers and
                            integrators tie revenue to average tokens, but their costs are driven by highly variable
                            usage.</p>

                        <p>The <a href="https://x.com/edzitron/status/1942274753265557918">narrative</a> that "Cursor
                            raised prices because Anthropic raised theirs" gets the causality backwards. Cursor's users
                            generated the extreme cost tail, because they had no usage limits in place; Anthropic's
                            response was a downstream response, and Cursor's price hikes were forced re-pricings of the
                            tail risk. Anthropic enforced new weekly limits after Cursor's re-pricings, indicating they
                            still could not keep up with the demand.</p>

                        <p>Each movement is a pressurizing force. The model providers can either absorb the cost shock,
                            or pass it downstream to integrators. The integrators have the same choice (albeit with less
                            freedom).</p>

                        <p><strong>Both get squeezed:</strong></p>

                        <div class="image-container">
                            <img src="images/image3.png" alt="Demand elasticity and code generation usage">
                        </div>

                        <p>The dynamic is especially concerning for startups who face fat tail risk, and have no margin
                            buffer to absorb it.</p>

                        <p>
                            <a href="https://sfcompute.com/inference">SF Compute</a>'s pricing section is the first
                            vendor statement I've seen
                            that acknowledges the token is not a fixed unit of cost. Their market-based prices expose
                            the real cost drivers
                            of inference (GPU supply, hardware efficiency etc.), in order to offer cheaper prices to
                            developers.
                        </p>

                        <p>
                            As usage grows, though, the narrative flips from "cheaper on average" to "probable
                            insolvency."
                            Hand-wavey explanations like “power users” or “GPU scarcity” for driving the cost tail
                            misses the point: varying costs across different requests is the true issue, and necessarily
                            constrains demand.
                        </p>

                        <p>
                            Said another way: under finite, static pricing models, no one can raise prices nor throttle
                            usage fast enough to keep up with the dynamic, growing cost curves.
                        </p>

                        <h1 id="saas-economics" class="section-head">Unpredictable costs kill SaaS-style margins</h1>

                        <p>Subscriptions misprice intelligence, and much of the industry recognizes this, but now we can
                            rigorously explain why.</p>

                        <p>Traditional SaaS pricing mirrors the physics of stable software, but AI introduces high
                            variance that breaks each of these laws.</p>

                        <div class="image-container">
                            <img src="images/image6.png" alt="Traditional SaaS vs AI pricing models comparison">
                        </div>
                        <p>(this can be its own article. If someone else writes it, I’ll link it here).</p>
                        <h1 id="resource-allocation" class="section-head">Scaling AI and improving unit economics require the same fix</h1>

                        <p>Anthropic endured <a href="https://x.com/shidoxo/status/1948767104557019336">constant
                                outages</a> <a href="https://x.com/mynamebedan/status/1945117071735480741">and</a>
                            service <a href="https://x.com/Krayorn/status/1957499186431070696">interruptions</a>, even
                            after <a href="https://x.com/MFrancis107/status/1948756484201779585">enforcing</a> rate
                            limits. This is inextricable from the fact that a token is not an atomic unit of cost.</p>

                        <p><strong>Because a token is also not an atomic unit of compute.</strong></p>

                        <p>Similar to how pricing structures treat a token as a fixed unit of cost, product interfaces
                            treat the prompt and context as a static artifact whose size we can measure. But the inputs
                            are more algorithmic than declarative; they dictate how much compute should be allocated to
                            a given task.</p>

                        <div class="image-container">
                            <img src="images/image4.png" alt="Token compute allocation">
                        </div>

                        <p>Soren likened applications like Cursor to <a
                                href="https://x.com/hypersoren/status/1941145741596217554">affiliate marketing</a> and
                            used that metaphor to demonstrate the inevitable <a
                                href="https://hypersoren.xyz/posts/smart-squeeze/">margin squeeze</a>.</p>

                        <p>But the affiliate metaphor holds all the way down the stack. Each layer is just a
                            pass-through to the scarce resource below it, and the scarcest resource under the current
                            paradigm is GPU capacity and power.</p>

                        <p>The most straightforward way to lift margin and/or prevent outages is to silently dial down
                            "intelligence": shorter chains, lower-quality responses, cheaper sampling. In late August,
                            many users <a href="https://x.com/TheAhmadOsman/status/1961326485672772040">accused</a>
                            Anthropic of doing exactly that after they saw malformed answers, broken tool calls, and
                            rising latency. Anthropic published an <a
                                href="https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues">incident
                                report</a>, but complaints about <a
                                href="https://www.reddit.com/r/ClaudeAI/comments/1mbsa4e/usage_limits_discussion_megathread_starting_july/?sort=confidence">degraded</a>
                            <a
                                href="https://www.reddit.com/r/ClaudeAI/comments/1mj0eyf/usage_limits_megathread_discussion_report_july_28/">quality</a>
                            and service <a
                                href="https://www.reddit.com/r/ClaudeAI/comments/1o2on6q/claude_code_context_window_issue/">continued</a>.
                        </p>

                        <p>Soren explained "everyone is executing a brute-force/blunt 'hidden work' optimization: what's
                            the least amount of compute I can get away with using such that my user doesn't churn?"</p>

                        <p>This is starting to sound like a market for lemons, where quality is unobservable, and price
                            competition drives average quality to the floor.</p>

                        <p>The problem is that this degradation of service is hidden from users, so they have no
                            incentive to optimize their requests to minimize compute. And users repeatedly <a
                                href="https://www.reddit.com/r/ClaudeAI/comments/1mj0eyf/usage_limits_megathread_discussion_report_july_28/">ask</a>
                            for visibility: "Just show a dashboard with remaining weekly & Opus—stop making us guess."
                        </p>

                        <p>But providers can't offer this, because costs are unpredictable for them too. It's invisible
                            to everyone until the moment the system <a
                                href="https://www.reddit.com/r/RooCode/comments/1lo5cyd/claude_code_crashes/">crashes</a>
                            without warning, leaving users frustrated and providers blindsided by the bill.</p>

                        <p>Usage caps are the resource allocator of last resort, but they're blunt. Unless the provider
                            can reject or throttle requests before it starts executing, the cost from the exploding
                            compute still materializes. The choice is who should shoulder it.</p>

                        <h1 id="solutions" class="section-head">Towards a new resource allocation (and pricing) paradigm
                        </h1>

                        <p>
                            High variance in costs necessarily constrains demand; today, the constraints are reactive.
                        </p>

                        <p>
                            To safely cushion from unbounded costs, a business model must price in the variance or be
                            well above the true cost
                            on average. Ideally by anchoring price to value delivered instead of token cost; but value
                            delivered also happens
                            to be highly variable and subjective. At the same time, there&apos;s structure to value:
                            reliability, relevance, actionability.
                        </p>

                        <p>
                            The key insight is that margin squeeze and resource misallocation are two sides of the same
                            problem. Solving one side of the equation should solve the other. If you can measure the
                            value delivered, you can price that instead of raw compute. And if you can price outcomes in
                            terms of value delivered, you can budget the exact amount of compute and data that maximizes
                            profit on each task.
                        </p>

                        <p>
                            So the layer that owns the meter also decides how much compute and data to deploy and keeps
                            the spread between cost and price. Today that meter sits inside the model; tomorrow it could
                            sit inside an orchestrator that plans the whole workflow.
                        </p>

                        <p>
                            The implication is technologic, not economic: the scaling prize may go to whoever defines
                            and measures “intelligence value,” not necessarily whoever trains the biggest, most capable
                            model.
                        </p>
                        <p style="font-style: italic; text-align: center; color: rgb(102, 102, 102);" class="mb-0!">
                            Soren Larson, @trickylabyrinth and Judah contributed significantly to this piece.
                        </p>


                    </div>
                </div>

                <div class="max-w-[190px] border-l flex-grow-0 flex-shrink-0 hidden dark:border-slate-700 xl:block">

                    <div class="
                    mb-3
                    pl-3
                    pb-5
                    prose prose-sm dark:prose-invert
                    prose-li:my-2

                    prose-ol:prose-li:list-[lower-alpha] prose-ol:prose-li:my-2
                    prose-a:prose-li:prose-li:font-normal prose-li:prose-li:my-1 prose-li:prose-li:text-[0.65rem]

                    prose-ol:prose-li:prose-li:list-[lower-roman]
                    prose-li:prose-li:prose-li:text-[0.6rem]

                    sticky top-5
                    text-xs
                ">
                        <div class="table-of-contents">
                            <h3>Table of Contents</h3>
                            <ol>
                                <li><a href="#intro">Intro</a></li>
                                <li><a href="#token-not-fixed">A token is not a fixed, atomic unit of cost</a></li>
                                <li><a href="#variance-scales">And tail risk compounds with usage</a></li>
                                <li><a href="#tail-risk">Margins collapse at hyperscale</a></li>
                                <li><a href="#tail-risk" class="ml-[20px] text-xs">Formalization</a></li>
                                <li><a href="#margins-collapse">High variance forces startups to constrain demand</a></li>
                                <li><a href="#saas-economics">Unpredictable costs kill SaaS-style margins</a>
                                </li>
                                <li><a href="#resource-allocation">Scaling AI and improving unit economics require the same fix</a></li>
                                <li><a href="#solutions">Towards a new resource allocation (and pricing) paradigm</a></li>
                                <li><a href="#solutions">Acknowledgements</a></li>
                            </ol>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </div>

    <div class="border-t border-b p-8 w-vp dark:border-slate-700">
        <div class="container max-w-[650px] mx-auto footer">
            <div class="hyphens-auto
                max-w-none
                prose prose-sm dark:prose-invert
                prose-a:border-b prose-a:border-slate-500 prose-a:font-sans prose-a:no-underline
                hover:prose-a:border-b-slate-200
                prose-code:prose-a:text-sm prose-code:prose-p:text-sm
                prose-p:font-serif
                prose-strong:font-sans
                ">

                <!-- <p><sup id="footnote-1"><a href="">1</a></sup> Footnotes go here</p>                 -->

                <p style="font-style: italic; text-align: center; color: rgb(102, 102, 102); font-size: 0.9rem;">
                    Thank you to <a href="https://x.com/hypersoren">@hypersoren</a>, <a
                        href="https://x.com/trickylabyrinth">@trickylabyrinth</a> and <a
                        href="https://joodaloop.com/">Judah</a> for their work, and to Jason Harrison, Anthony Crognale,
                    Suraj Srivats, Ade Oshineye and Alex Komoroske for feedback on early versions of this.
                    <br><br>And to <a href="https://brandur.org/">Brandur</a>, for the layout inspiration.<br>
                    <!-- And to Analogue Group, for the push to set out on this expedition. -->
                </p>
            </div>

        </div>
    </div>
    </div>




    <!-- <div class="border-t pb-20 p-8 w-vp dark:border-slate-700">
        <div class="container max-w-[650px] mx-auto">
            <div class="hyphens-auto
            italic
            max-w-none
            prose prose-sm dark:prose-invert
            prose-a:border-b prose-a:border-slate-500 prose-a:font-sans prose-a:no-underline
            hover:prose-a:border-b-slate-200
            prose-p:font-serif
            ">

                <p>
                    Acknowledgements go here.
                </p>
<p style="font-style: italic; text-align: center; color: rgb(102, 102, 102); font-size: 0.9rem;">
    Thank you to <a href="https://x.com/hypersoren">@hypersoren</a>, <a href="https://x.com/trickylabyrinth">@trickylabyrinth</a> and <a href="https://joodaloop.com/">Judah</a> for their work, and to Jason Harrison, Anthony Crognale, Suraj Srivats, Ade Oshineye and Alex Komoroske for offering feedback on early versions of this. And to Analogue Group, for the push to set out on this expedition.
</p>
            </div>
        </div>
    </div> -->
    <footer>
        <p style="font-size: 0.9rem; text-align: center; color: rgba(0, 0, 0, 0.6); margin: 4rem 0;">
            It's never too late to
            <span id="scrollLink" style="opacity: 1; color: #0033cc; text-decoration: underline; cursor: pointer;">
                start over</span>.
        </p>
    </footer>
    <button id="backToTop" style="display: block;">↑ Top</button>
</body>
<script>
// Scroll to top functionality
document.getElementById('backToTop').addEventListener('click', function() {
    window.scrollTo({top: 0, behavior: 'smooth'});
});

document.getElementById('scrollLink').addEventListener('click', function() {
    window.scrollTo({top: 0, behavior: 'smooth'});
});

// Show/hide back to top button
window.addEventListener('scroll', function() {
    const backToTop = document.getElementById('backToTop');
    if (window.pageYOffset > 300) {
        backToTop.style.display = 'block';
    } else {
        backToTop.style.display = 'none';
    }
});
</script>
</html>